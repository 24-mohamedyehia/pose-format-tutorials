{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7e1834",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Extracting Landmarks from a Single Video with world landmarks\n",
    "\n",
    "### Method 1: Using the Command Line (CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CLI to convert a video to a .pose file\n",
    "# Change the path according to your file\n",
    "!video_to_pose --format mediapipe -i \"path/to/your/video.mp4\" -o \"output.pose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With additional settings for higher accuracy\n",
    "!video_to_pose --format mediapipe -i \"path/to/your/video.mp4\" -o \"output_hq.pose\" \\\n",
    "  --additional-config=\"model_complexity=2,smooth_landmarks=false,refine_face_landmarks=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c1d9a",
   "metadata": {},
   "source": [
    "### Method 2: Using Python Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f524a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pose_format import Pose\n",
    "from pose_format.utils.holistic import load_holistic\n",
    "from simple_video_utils.metadata import video_metadata\n",
    "from simple_video_utils.frames import read_frames_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a46f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_pose_from_video(video_path: str, output_path: str, model_complexity: int = 1):\n",
    "    \"\"\"\n",
    "        Extract landmarks from a video and save them as a .pose file\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        video_path : str\n",
    "            Path to the input video\n",
    "        output_path : str\n",
    "            Directory to save the output .pose file\n",
    "        model_complexity : int\n",
    "            Model complexity (0, 1, or 2). 2 = highest accuracy but slower\n",
    "        \"\"\"\n",
    "    print(f\"üé¨ Loading video: {video_path}\")\n",
    "    \n",
    "    # Read video metadata\n",
    "    metadata = video_metadata(video_path)\n",
    "    width = metadata.width\n",
    "    height = metadata.height\n",
    "    fps = metadata.fps\n",
    "    \n",
    "    print(f\"üìê Dimensions: {width}x{height}\")\n",
    "    print(f\"üéûÔ∏è Frame rate: {fps} FPS\")\n",
    "    \n",
    "    # Read video frames\n",
    "    frames = read_frames_exact(video_path)\n",
    "    \n",
    "    # MediaPipe Holistic settings\n",
    "    holistic_config = {\n",
    "        'model_complexity': model_complexity,\n",
    "        'smooth_landmarks': True,\n",
    "        'refine_face_landmarks': True  # for iris landmarks\n",
    "    }\n",
    "    \n",
    "    print(\"‚öôÔ∏è Extracting landmarks...\")\n",
    "    \n",
    "    # Extract pose\n",
    "    pose = load_holistic(\n",
    "        frames,\n",
    "        fps=fps,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        progress=True,  # Show progress bar\n",
    "        additional_holistic_config=holistic_config\n",
    "    )\n",
    "    \n",
    "    # Save result\n",
    "    print(f\"üíæ Saving to: {output_path}\")\n",
    "    file_name = os.path.basename(video_path)\n",
    "    name, _ = os.path.splitext(file_name)\n",
    "    output_path = os.path.join(output_path, f\"{name}.pose\")\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pose.write(f)\n",
    "    \n",
    "    print(\"‚úÖ Done!\")\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa459cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Loading video: ../data/original_videos/example.mp4\n",
      "üìê Dimensions: 640x480\n",
      "üéûÔ∏è Frame rate: 24.166666666666668 FPS\n",
      "‚öôÔ∏è Extracting landmarks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [00:40,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving to: ../data/pose_files/\n",
      "‚úÖ Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Change the paths according to your files\n",
    "\n",
    "VIDEO_PATH = \"../data/original_videos/example.mp4\"\n",
    "OUTPUT_PATH = \"../data/pose_files/\"\n",
    "\n",
    "# Extract pose\n",
    "pose = extract_pose_from_video(VIDEO_PATH, OUTPUT_PATH, model_complexity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05865f39",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Extracting Landmarks from a Folder of Videos without world landmarks\n",
    "\n",
    "### Method 1: Using the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process a folder with multiple workers (parallel processing)\n",
    "!videos_to_poses --format mediapipe --directory \"/path/to/videos\" --num-workers 4 --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172300dd",
   "metadata": {},
   "source": [
    "### Method 2: Using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ec8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "from pose_format import Pose\n",
    "from pose_format.utils.holistic import load_holistic\n",
    "from pose_format.utils.openpose import load_openpose_directory\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47192430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe Holistic face mesh contour points\n",
    "mp_holistic = mp.solutions.holistic\n",
    "FACEMESH_CONTOURS_POINTS = [str(p) for p in sorted(set([p for p_tup in list(mp_holistic.FACEMESH_CONTOURS) for p in p_tup]))]\n",
    "\n",
    "SUPPORTED_VIDEO_FORMATS = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".flv\", \".wmv\", \".webm\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c528643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames(cap: cv2.VideoCapture):\n",
    "    \"\"\"Generator: yields frames as RGB numpy arrays from an open VideoCapture.\"\"\"\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        yield cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    cap.release()\n",
    "\n",
    "def find_videos(directory: str, recursive: bool = False):\n",
    "    directory = Path(directory)\n",
    "    glob_method = directory.rglob if recursive else directory.glob\n",
    "    videos = []\n",
    "    for ext in SUPPORTED_VIDEO_FORMATS:\n",
    "        videos.extend(glob_method(f\"*{ext}\"))\n",
    "    return sorted(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ce1b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video(video_path: Path, output_dir: Path, model_complexity: int = 1, reduce: bool = False):\n",
    "    \"\"\"\n",
    "    Process a single video and save the result in the specified folder.\n",
    "    Implements full pose estimation inline (opens video, runs load_holistic, writes .pose).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_path = output_dir / f\"{video_path.stem}.pose\"\n",
    "\n",
    "        # Skip if file exists\n",
    "        if output_path.exists():\n",
    "            return f\"‚è≠Ô∏è Skipped (exists): {video_path.name}\"\n",
    "\n",
    "        # Fallback: open with cv2.VideoCapture and stream frames to load_holistic\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            return f\"‚ùå Failed {video_path.name}: cannot open video\"\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "\n",
    "        frames_gen = load_video_frames(cap)\n",
    "\n",
    "        additional_holistic_config = {\n",
    "            'model_complexity': model_complexity,\n",
    "            'smooth_landmarks': True,\n",
    "            'refine_face_landmarks': True\n",
    "        }\n",
    "\n",
    "        pose = load_holistic(\n",
    "            frames_gen,\n",
    "            fps=fps,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            progress=False,\n",
    "            additional_holistic_config=additional_holistic_config\n",
    "        )\n",
    "\n",
    "        # Keep core 2D components\n",
    "        pose = pose.get_components([\"POSE_LANDMARKS\", \"FACE_LANDMARKS\", \"LEFT_HAND_LANDMARKS\", \"RIGHT_HAND_LANDMARKS\"])\n",
    "\n",
    "        if reduce:\n",
    "            pose = pose.get_components(\n",
    "                [\"POSE_LANDMARKS\", \"FACE_LANDMARKS\", \"LEFT_HAND_LANDMARKS\", \"RIGHT_HAND_LANDMARKS\"],\n",
    "                {\"FACE_LANDMARKS\": FACEMESH_CONTOURS_POINTS}\n",
    "            )\n",
    "\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pose.write(f)\n",
    "\n",
    "        return f\"‚úÖ Done: {video_path.name}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Failed {video_path.name}: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d48de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_process_videos(directory: str, output_dir: str, recursive: bool = False, num_workers: int = 1, model_complexity: int = 1, reduce: bool = False):\n",
    "    \"\"\"\n",
    "    Batch process a folder of videos in parallel.\n",
    "    \"\"\"\n",
    "    videos = find_videos(directory, recursive)\n",
    "    print(f\"üìÅ Found {len(videos)} videos in {directory}\")\n",
    "    if not videos:\n",
    "        print(\"‚ö†Ô∏è No videos found.\")\n",
    "        return\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(process_single_video, v, output_dir, model_complexity, reduce): v for v in videos}\n",
    "        for future in tqdm(as_completed(futures), total=len(videos), desc=\"Processing\"):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "            print(result)\n",
    "\n",
    "    success = sum(1 for r in results if r.startswith(\"‚úÖ\"))\n",
    "    skipped = sum(1 for r in results if r.startswith(\"‚è≠Ô∏è\"))\n",
    "    failed = sum(1 for r in results if r.startswith(\"‚ùå\"))\n",
    "\n",
    "    print(\"\\nüìä Statistics:\")\n",
    "    print(f\"   ‚úÖ Success: {success}\")\n",
    "    print(f\"   ‚è≠Ô∏è Skipped: {skipped}\")\n",
    "    print(f\"   ‚ùå Failed: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f593f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 14 videos in ../data/original_videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipped (exists): 554365435.mp4\n",
      "‚è≠Ô∏è Skipped (exists): SGB_FSS_rotation_left.mp4\n",
      "‚è≠Ô∏è Skipped (exists): SGB_FSS_original.mp4\n",
      "‚è≠Ô∏è Skipped (exists): SGB_FSS_rotation_right.mp4\n",
      "‚è≠Ô∏è Skipped (exists): SGB_FSS_skew_left .mp4\n",
      "‚è≠Ô∏è Skipped (exists): SGB_FSS_skew_right.mp4\n",
      "‚è≠Ô∏è Skipped (exists): SGB_FSS_zoomin.mp4\n",
      "‚è≠Ô∏è Skipped (exists): SGB_FSS_zoomout .mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:00<00:00, 69.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipped (exists): ÿ≠ÿ®.mp4\n",
      "‚è≠Ô∏è Skipped (exists): ÿ≠ŸÖŸÑ.mp4\n",
      "‚è≠Ô∏è Skipped (exists): ÿ≤ŸÅÿßŸÅ.mp4\n",
      "‚è≠Ô∏è Skipped (exists): ÿ∑ŸÑÿßŸÇ.mp4\n",
      "‚è≠Ô∏è Skipped (exists): ŸÇÿ±Ÿäÿ®.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:52<00:00,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done: SGB_FSS.mp4\n",
      "\n",
      "üìä Statistics:\n",
      "   ‚úÖ Success: 1\n",
      "   ‚è≠Ô∏è Skipped: 13\n",
      "   ‚ùå Failed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VIDEOS_DIRECTORY = \"../data/original_videos\"\n",
    "OUTPUT_DIR = \"../data/pose_files\"\n",
    "\n",
    "batch_process_videos(\n",
    "    directory=VIDEOS_DIRECTORY,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    recursive=True,\n",
    "    num_workers=2,\n",
    "    model_complexity=2,\n",
    "    reduce=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ebf21",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Understanding the Extracted Data Structure\n",
    "\n",
    "### MediaPipe Holistic extracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09aa417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to understand the extracted data\n",
    "from pose_format import Pose\n",
    "\n",
    "def analyze_pose_file(pose_path: str):\n",
    "    \"\"\"\n",
    "    Analyze a pose file and display its information\n",
    "    \"\"\"\n",
    "    with open(pose_path, 'rb') as f:\n",
    "        pose = Pose.read(f.read())\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä Pose File Information\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Header Information\n",
    "    print(f\"\\nüìê Dimensions:\")\n",
    "    print(f\"   Width: {pose.header.dimensions.width}\")\n",
    "    print(f\"   Height: {pose.header.dimensions.height}\")\n",
    "    print(f\"   Depth: {pose.header.dimensions.depth}\")\n",
    "    \n",
    "    # Body Information\n",
    "    print(f\"\\nüéûÔ∏è Body Information:\")\n",
    "    print(f\"   FPS: {pose.body.fps} FPS\")\n",
    "    print(f\"   Data Shape: {pose.body.data.shape}\")\n",
    "    print(f\"   (Frames, People, Points, Dimensions)\")\n",
    "    \n",
    "    frames, people, points, dims = pose.body.data.shape\n",
    "    print(f\"\\n   üìπ Number of Frames: {frames}\")\n",
    "    print(f\"   üë• Number of People: {people}\")\n",
    "    print(f\"   üìç Total Number of Points: {points}\")\n",
    "    print(f\"   üìè Number of Dimensions: {dims} (X, Y, Z)\")\n",
    "    \n",
    "    # Components\n",
    "    print(f\"\\nüß© Components:\")\n",
    "    total_points = 0\n",
    "    for i, comp in enumerate(pose.header.components):\n",
    "        print(f\"\\n   {i+1}. {comp.name}\")\n",
    "        print(f\"      Points: {len(comp.points)}\")\n",
    "        print(f\"      Limbs: {len(comp.limbs)}\")\n",
    "        print(f\"      Format: {comp.format}\")\n",
    "        total_points += len(comp.points)\n",
    "    \n",
    "    print(f\"\\n   üìç Total Points: {total_points}\")\n",
    "    \n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "870e9483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä Pose File Information\n",
      "============================================================\n",
      "\n",
      "üìê Dimensions:\n",
      "   Width: 640\n",
      "   Height: 480\n",
      "   Depth: 0\n",
      "\n",
      "üéûÔ∏è Body Information:\n",
      "   FPS: 24.0 FPS\n",
      "   Data Shape: (133, 1, 203, 3)\n",
      "   (Frames, People, Points, Dimensions)\n",
      "\n",
      "   üìπ Number of Frames: 133\n",
      "   üë• Number of People: 1\n",
      "   üìç Total Number of Points: 203\n",
      "   üìè Number of Dimensions: 3 (X, Y, Z)\n",
      "\n",
      "üß© Components:\n",
      "\n",
      "   1. POSE_LANDMARKS\n",
      "      Points: 33\n",
      "      Limbs: 35\n",
      "      Format: XYZC\n",
      "\n",
      "   2. FACE_LANDMARKS\n",
      "      Points: 128\n",
      "      Limbs: 196\n",
      "      Format: XYZC\n",
      "\n",
      "   3. LEFT_HAND_LANDMARKS\n",
      "      Points: 21\n",
      "      Limbs: 21\n",
      "      Format: XYZC\n",
      "\n",
      "   4. RIGHT_HAND_LANDMARKS\n",
      "      Points: 21\n",
      "      Limbs: 21\n",
      "      Format: XYZC\n",
      "\n",
      "   üìç Total Points: 203\n"
     ]
    }
   ],
   "source": [
    "# Analyze a pose file\n",
    "# Change the path if needed\n",
    "FILE_PATH = \"../data/pose_files/example.3.reduce.pose\"\n",
    "\n",
    "pose = analyze_pose_file(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b6724",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Accessing Specific Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fd3f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_specific_landmarks(pose: Pose):\n",
    "    \"\"\"\n",
    "    Get specific landmarks from the pose data\n",
    "    \"\"\"\n",
    "    # Raw data\n",
    "    data = pose.body.data          # Coordinates (frames, people, points, dims)\n",
    "    confidence = pose.body.confidence  # Confidence scores (frames, people, points)\n",
    "    \n",
    "    # Get specific component\n",
    "    # Example: Get only body landmarks\n",
    "    body_pose = pose.get_components([\"POSE_LANDMARKS\"])\n",
    "    print(f\"Body landmarks only: {body_pose.body.data.shape}\")\n",
    "    \n",
    "    # Example: Get only hand landmarks\n",
    "    hands_pose = pose.get_components([\"LEFT_HAND_LANDMARKS\", \"RIGHT_HAND_LANDMARKS\"])\n",
    "    print(f\"Hand landmarks only: {hands_pose.body.data.shape}\")\n",
    "    \n",
    "    # Access specific landmark by name\n",
    "    # Example: Get index of the nose landmark\n",
    "    nose_index = pose.header.get_point_index(\"POSE_LANDMARKS\", \"NOSE\")\n",
    "    print(f\"\\nIndex of the nose landmark: {nose_index}\")\n",
    "    \n",
    "    # Nose coordinates in all frames\n",
    "    nose_coords = data[:, 0, nose_index, :]  # (frames, 3)\n",
    "    print(f\"Shape of nose coordinates: {nose_coords.shape}\")\n",
    "    \n",
    "    # Example: Get right wrist landmark\n",
    "    right_wrist_index = pose.header.get_point_index(\"POSE_LANDMARKS\", \"RIGHT_WRIST\")\n",
    "    right_wrist_coords = data[:, 0, right_wrist_index, :]\n",
    "    print(f\"\\nRight wrist coordinates in the first 5 frames:\")\n",
    "    print(right_wrist_coords[:5])\n",
    "    \n",
    "    return data, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a637856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body landmarks only: (42, 1, 33, 3)\n",
      "Hand landmarks only: (42, 1, 42, 3)\n",
      "\n",
      "Index of the nose landmark: 0\n",
      "Shape of nose coordinates: (42, 3)\n",
      "\n",
      "Right wrist coordinates in the first 5 frames:\n",
      "[[762.5533447265625 833.9669799804688 -0.924421489238739]\n",
      " [750.7291870117188 827.2061157226562 -0.9257319569587708]\n",
      " [736.3405151367188 819.6056518554688 -0.932299017906189]\n",
      " [726.1067504882812 815.055908203125 -0.9344683885574341]\n",
      " [723.3048706054688 811.894775390625 -0.8845396041870117]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "data, confidence = get_specific_landmarks(pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b8378",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ List of MediaPipe Holistic Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d0254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body points: 33\n",
      "Hand points (one hand): 21\n",
      "Face points: 468 (or 478 with iris)\n",
      "\n",
      "Total: 33 + 468 + 21 + 21 + 33 = 576 (without iris) or 586 (with iris)\n"
     ]
    }
   ],
   "source": [
    "# Body points (33 points)\n",
    "BODY_POINTS = [\n",
    "    'NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER',\n",
    "    'RIGHT_EYE_INNER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "    'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT',\n",
    "    'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW',\n",
    "    'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY',\n",
    "    'LEFT_INDEX', 'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB',\n",
    "    'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE',\n",
    "    'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL', 'RIGHT_HEEL',\n",
    "    'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX'\n",
    "]\n",
    "\n",
    "# Hand points (21 points per hand)\n",
    "HAND_POINTS = [\n",
    "    'WRIST', 'THUMB_CMC', 'THUMB_MCP', 'THUMB_IP', 'THUMB_TIP',\n",
    "    'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_DIP', 'INDEX_FINGER_TIP',\n",
    "    'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_TIP',\n",
    "    'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_DIP', 'RING_FINGER_TIP',\n",
    "    'PINKY_MCP', 'PINKY_PIP', 'PINKY_DIP', 'PINKY_TIP'\n",
    "]\n",
    "\n",
    "# Face points (468 or 478 points with iris)\n",
    "print(f\"Body points: {len(BODY_POINTS)}\")\n",
    "print(f\"Hand points (one hand): {len(HAND_POINTS)}\")\n",
    "print(f\"Face points: 468 (or 478 with iris)\")\n",
    "print(f\"\\nTotal: 33 + 468 + 21 + 21 + 33 = 576 (without iris) or 586 (with iris)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose-format-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
