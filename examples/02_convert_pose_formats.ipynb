{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5214778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pose_format import Pose\n",
    "from pose_format.numpy import NumPyPoseBody\n",
    "from pose_format.pose_header import PoseHeader, PoseHeaderComponent, PoseHeaderDimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296cdc6",
   "metadata": {},
   "source": [
    "## üì• Read the .pose file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfdc9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pose_file(pose_path: str) -> Pose:\n",
    "    \"\"\"\n",
    "    Reads a .pose file and returns a Pose object.\n",
    "    \"\"\"\n",
    "    with open(pose_path, 'rb') as f:\n",
    "        pose = Pose.read(f.read())\n",
    "    return pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88d67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pose = '../data/pose_files/example.pose'\n",
    "pose = load_pose_file(path_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9d600",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Save as .pose (original format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56d426cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../output/convert_pose_formats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8aac15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: ../output/convert_pose_formats/output.pose\n"
     ]
    }
   ],
   "source": [
    "def save_as_pose(pose: Pose, output_path: str):\n",
    "    \"\"\"\n",
    "    Saves a Pose object to a .pose file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pose.write(f)\n",
    "    print(f\"‚úÖ Saved to: {output_path}\")\n",
    "\n",
    "\n",
    "save_as_pose(pose, f\"{save_path}/output.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b737d5",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Convert to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4631b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_json(pose: Pose, output_path: str = None, include_header: bool = True):\n",
    "    \"\"\"\n",
    "    Convert Pose to JSON.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pose : Pose\n",
    "        Pose object.\n",
    "    output_path : str, optional\n",
    "        Output file path. If not provided, returns the dict only.\n",
    "    include_header : bool\n",
    "        Include header information.\n",
    "    \"\"\"\n",
    "    # Convert data to Python lists\n",
    "    data = pose.body.data.filled(0).tolist()  # replace missing values with 0\n",
    "    confidence = pose.body.confidence.tolist()\n",
    "    \n",
    "    result = {\n",
    "        \"data\": data,\n",
    "        \"confidence\": confidence,\n",
    "        \"fps\": pose.body.fps,\n",
    "        \"shape\": {\n",
    "            \"frames\": pose.body.data.shape[0],\n",
    "            \"people\": pose.body.data.shape[1],\n",
    "            \"points\": pose.body.data.shape[2],\n",
    "            \"dimensions\": pose.body.data.shape[3]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if include_header:\n",
    "        result[\"header\"] = {\n",
    "            \"version\": pose.header.version,\n",
    "            \"dimensions\": {\n",
    "                \"width\": pose.header.dimensions.width,\n",
    "                \"height\": pose.header.dimensions.height,\n",
    "                \"depth\": pose.header.dimensions.depth\n",
    "            },\n",
    "            \"components\": [\n",
    "                {\n",
    "                    \"name\": comp.name,\n",
    "                    \"points\": comp.points,\n",
    "                    \"format\": comp.format,\n",
    "                    \"limbs\": comp.limbs,\n",
    "                    \"num_points\": len(comp.points)\n",
    "                }\n",
    "                for comp in pose.header.components\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    if output_path:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        print(f\"‚úÖ Saved to: {output_path}\")\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f0f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: ../output/convert_pose_formats/output.json\n"
     ]
    }
   ],
   "source": [
    "json_data = pose_to_json(pose, f\"{save_path}/output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b378908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_header_from_dict(header_dict: dict) -> PoseHeader:\n",
    "    \"\"\"\n",
    "    Build a PoseHeader from a JSON header dictionary.\n",
    "    \"\"\"\n",
    "    dimensions = PoseHeaderDimensions(\n",
    "        width=header_dict[\"dimensions\"][\"width\"],\n",
    "        height=header_dict[\"dimensions\"][\"height\"],\n",
    "        depth=header_dict[\"dimensions\"].get(\"depth\", 0)\n",
    "    )\n",
    "    components = []\n",
    "    for comp in header_dict.get(\"components\", []):\n",
    "        limbs = [tuple(limb) for limb in comp.get(\"limbs\", [])]\n",
    "        colors = comp.get(\"colors\")\n",
    "        if not colors:\n",
    "            colors = [(255, 255, 255)] * len(limbs)\n",
    "        components.append(\n",
    "            PoseHeaderComponent(\n",
    "                name=comp[\"name\"],\n",
    "                points=comp[\"points\"],\n",
    "                limbs=limbs,\n",
    "                colors=colors,\n",
    "                point_format=comp.get(\"format\", \"XYZC\")\n",
    "            )\n",
    "        )\n",
    "    return PoseHeader(\n",
    "        version=header_dict.get(\"version\", 0.1),\n",
    "        dimensions=dimensions,\n",
    "        components=components\n",
    "    )\n",
    "\n",
    "def clone_header(\n",
    "    reference_header: PoseHeader,\n",
    "    width: int = None,\n",
    "    height: int = None,\n",
    "    depth: int = None,\n",
    "    version: float = None\n",
    " ) -> PoseHeader:\n",
    "    \"\"\"\n",
    "    Create a new header based on a reference header, with optional overrides.\n",
    "    \"\"\"\n",
    "    dimensions = PoseHeaderDimensions(\n",
    "        width if width is not None else reference_header.dimensions.width,\n",
    "        height if height is not None else reference_header.dimensions.height,\n",
    "        depth if depth is not None else reference_header.dimensions.depth\n",
    "    )\n",
    "    return PoseHeader(\n",
    "        version=version if version is not None else reference_header.version,\n",
    "        dimensions=dimensions,\n",
    "        components=reference_header.components,\n",
    "        is_bbox=getattr(reference_header, \"is_bbox\", False)\n",
    "    )\n",
    "\n",
    "def build_pose_from_arrays(\n",
    "    data: np.ndarray,\n",
    "    confidence: np.ndarray,\n",
    "    fps: float,\n",
    "    header: PoseHeader,\n",
    "    mask: np.ndarray = None\n",
    " ) -> Pose:\n",
    "    \"\"\"\n",
    "    Build a Pose object from raw arrays.\n",
    "    IMPORTANT: Uses NumPyPoseBody to ensure proper write() functionality.\n",
    "    \"\"\"\n",
    "    if mask is not None:\n",
    "        data = np.ma.MaskedArray(data, mask=mask)\n",
    "    # Use NumPyPoseBody instead of generic PoseBody for write support\n",
    "    body = NumPyPoseBody(fps, data, confidence)\n",
    "    return Pose(header, body)\n",
    "\n",
    "def json_to_pose(\n",
    "    json_path: str,\n",
    "    output_path: str,\n",
    "    reference_pose: Pose = None\n",
    " ) -> Pose:\n",
    "    \"\"\"\n",
    "    Convert JSON back to a .pose file.\n",
    "    If the JSON does not include a header, a reference pose is required.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    header_dict = json_data.get(\"header\")\n",
    "    if header_dict:\n",
    "        header = build_header_from_dict(header_dict)\n",
    "    elif reference_pose is not None:\n",
    "        header = reference_pose.header\n",
    "    else:\n",
    "        raise ValueError(\"Header is missing. Provide a reference_pose.\")\n",
    "\n",
    "    data = np.array(json_data[\"data\"], dtype=np.float32)\n",
    "    confidence = np.array(json_data[\"confidence\"], dtype=np.float32)\n",
    "    fps = float(json_data.get(\"fps\", reference_pose.body.fps if reference_pose else 30))\n",
    "    pose = build_pose_from_arrays(data, confidence, fps, header)\n",
    "    save_as_pose(pose, output_path)\n",
    "    return pose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57947ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: ../output/convert_pose_formats/restored_from_json.pose\n"
     ]
    }
   ],
   "source": [
    "# Example: JSON -> .pose\n",
    "restored_pose_json = json_to_pose(f\"{save_path}/output.json\", f\"{save_path}/restored_from_json.pose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea87d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_json_compact(pose: Pose, output_path: str):\n",
    "    \"\"\"\n",
    "    Convert Pose to compact JSON (includes full header for standalone conversion).\n",
    "    \"\"\"\n",
    "    data = pose.body.data.filled(0).tolist()\n",
    "    confidence = pose.body.confidence.tolist()\n",
    "    \n",
    "    # Include full header for independent reconstruction\n",
    "    result = {\n",
    "        \"d\": data,\n",
    "        \"c\": confidence,\n",
    "        \"f\": float(pose.body.fps),\n",
    "        \"h\": {\n",
    "            \"v\": float(pose.header.version),\n",
    "            \"w\": int(pose.header.dimensions.width),\n",
    "            \"h\": int(pose.header.dimensions.height),\n",
    "            \"d\": int(pose.header.dimensions.depth),\n",
    "            \"comps\": [\n",
    "                {\n",
    "                    \"n\": comp.name,\n",
    "                    \"pts\": comp.points,\n",
    "                    \"fmt\": comp.format,\n",
    "                    \"limbs\": [[int(x) for x in limb] if hasattr(limb, '__iter__') else int(limb) for limb in comp.limbs],\n",
    "                    \"colors\": [[int(x) for x in c] if hasattr(c, '__iter__') else int(c) for c in getattr(comp, \"colors\", [])] if hasattr(comp, \"colors\") else None\n",
    "                }\n",
    "                for comp in pose.header.components\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, separators=(',', ':'))\n",
    "    \n",
    "    print(f\"‚úÖ Saved (compact) to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6960615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_compact_to_pose(\n",
    "    json_path: str,\n",
    "    output_path: str,\n",
    "    reference_pose: Pose = None\n",
    " ) -> Pose:\n",
    "    \"\"\"\n",
    "    Convert compact JSON back to a .pose file (now standalone - no reference needed).\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        compact = json.load(f)\n",
    "\n",
    "    data = np.array(compact[\"d\"], dtype=np.float32)\n",
    "    confidence = np.array(compact[\"c\"], dtype=np.float32)\n",
    "    fps = float(compact[\"f\"])\n",
    "    \n",
    "    # Reconstruct header from embedded data\n",
    "    header_data = compact.get(\"h\")\n",
    "    if header_data:\n",
    "        # Build header from embedded info\n",
    "        dimensions = PoseHeaderDimensions(\n",
    "            width=header_data[\"w\"],\n",
    "            height=header_data[\"h\"],\n",
    "            depth=header_data.get(\"d\", 0)\n",
    "        )\n",
    "        components = []\n",
    "        for comp in header_data.get(\"comps\", []):\n",
    "            limbs = [tuple(limb) for limb in comp.get(\"limbs\", [])]\n",
    "            colors = comp.get(\"colors\")\n",
    "            if not colors:\n",
    "                colors = [(255, 255, 255)] * len(limbs)\n",
    "            components.append(\n",
    "                PoseHeaderComponent(\n",
    "                    name=comp[\"n\"],\n",
    "                    points=comp[\"pts\"],\n",
    "                    limbs=limbs,\n",
    "                    colors=colors,\n",
    "                    point_format=comp.get(\"fmt\", \"XYZC\")\n",
    "                )\n",
    "            )\n",
    "        header = PoseHeader(\n",
    "            version=header_data.get(\"v\", 0.1),\n",
    "            dimensions=dimensions,\n",
    "            components=components\n",
    "        )\n",
    "    elif reference_pose is not None:\n",
    "        # Fallback to reference if no header\n",
    "        header = clone_header(\n",
    "            reference_header=reference_pose.header,\n",
    "            width=int(compact.get(\"w\", reference_pose.header.dimensions.width)),\n",
    "            height=int(compact.get(\"h\", reference_pose.header.dimensions.height))\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"No header found in compact JSON and no reference_pose provided.\")\n",
    "    \n",
    "    pose = build_pose_from_arrays(data, confidence, fps, header)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pose.write(f)\n",
    "    \n",
    "    return pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d96c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved (compact) to: ../output/convert_pose_formats/output.compact.json\n"
     ]
    }
   ],
   "source": [
    "# Example: compact JSON -> .pose (now standalone!)\n",
    "pose_to_json_compact(pose, f\"{save_path}/output.compact.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "219e5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_pose_compact = json_compact_to_pose(f\"{save_path}/output.compact.json\", f\"{save_path}/restored_from_compact.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327d685",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Convert to NumPy (.npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d78dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_npz(pose: Pose, output_path: str, compressed: bool = True):\n",
    "    \"\"\"\n",
    "    Convert Pose to NumPy (.npz) with full header information for standalone conversion.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pose : Pose\n",
    "        Pose object.\n",
    "    output_path : str\n",
    "        Output file path.\n",
    "    compressed : bool\n",
    "        Use compression (smaller size but slower read/write).\n",
    "    \"\"\"\n",
    "    # Extract data\n",
    "    data = np.array(pose.body.data.filled(0), dtype=np.float32)\n",
    "    confidence = np.array(pose.body.confidence, dtype=np.float32)\n",
    "    mask = np.array(pose.body.data.mask, dtype=bool)\n",
    "    \n",
    "    # Serialize components info as JSON string\n",
    "    components_data = []\n",
    "    for comp in pose.header.components:\n",
    "        components_data.append({\n",
    "            \"name\": comp.name,\n",
    "            \"points\": comp.points,\n",
    "            \"format\": comp.format,\n",
    "            \"limbs\": [[int(x) for x in limb] if hasattr(limb, '__iter__') else int(limb) for limb in comp.limbs],\n",
    "            \"colors\": [[int(x) for x in c] if hasattr(c, '__iter__') else int(c) for c in getattr(comp, \"colors\", [])] if hasattr(comp, \"colors\") else None\n",
    "        })\n",
    "    components_json = json.dumps(components_data)\n",
    "    \n",
    "    # Extra metadata\n",
    "    metadata = {\n",
    "        'fps': np.array([pose.body.fps]),\n",
    "        'width': np.array([pose.header.dimensions.width]),\n",
    "        'height': np.array([pose.header.dimensions.height]),\n",
    "        'depth': np.array([pose.header.dimensions.depth]),\n",
    "        'version': np.array([pose.header.version]),\n",
    "        'components': np.array([components_json], dtype=object)\n",
    "    }\n",
    "    \n",
    "    # Save\n",
    "    save_func = np.savez_compressed if compressed else np.savez\n",
    "    save_func(\n",
    "        output_path,\n",
    "        data=data,\n",
    "        confidence=confidence,\n",
    "        mask=mask,\n",
    "        **metadata\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Saved to: {output_path}\")\n",
    "    print(f\"   üì¶ File size: {Path(output_path).stat().st_size / 1024:.2f} KB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ae9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: ../output/convert_pose_formats/output.npz\n",
      "   üì¶ File size: 306.53 KB\n"
     ]
    }
   ],
   "source": [
    "pose_to_npz(pose, f\"{save_path}/output.npz\", compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3651273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npz_to_pose(npz_path: str, output_path: str, reference_pose: Pose = None) -> Pose:\n",
    "    \"\"\"\n",
    "    Convert NPZ back to a .pose file (now standalone - no reference needed).\n",
    "    \"\"\"\n",
    "    loaded = np.load(npz_path, allow_pickle=True)\n",
    "    \n",
    "    data = loaded['data'].astype(np.float32)\n",
    "    confidence = loaded['confidence'].astype(np.float32)\n",
    "    mask = loaded['mask'] if 'mask' in loaded.files else None\n",
    "    fps = float(loaded['fps'][0]) if 'fps' in loaded.files else 30.0\n",
    "    \n",
    "    # Try to load header from NPZ\n",
    "    if 'components' in loaded.files:\n",
    "        # Reconstruct header from saved components\n",
    "        width = int(loaded['width'][0]) if 'width' in loaded.files else data.shape[2]\n",
    "        height = int(loaded['height'][0]) if 'height' in loaded.files else data.shape[1]\n",
    "        depth = int(loaded['depth'][0]) if 'depth' in loaded.files else 0\n",
    "        version = float(loaded['version'][0]) if 'version' in loaded.files else 0.1\n",
    "        \n",
    "        dimensions = PoseHeaderDimensions(width, height, depth)\n",
    "        \n",
    "        # Deserialize components\n",
    "        components_json = str(loaded['components'][0])\n",
    "        components_data = json.loads(components_json)\n",
    "        \n",
    "        components = []\n",
    "        for comp_data in components_data:\n",
    "            limbs = [tuple(limb) for limb in comp_data.get(\"limbs\", [])]\n",
    "            colors = comp_data.get(\"colors\")\n",
    "            if not colors:\n",
    "                colors = [(255, 255, 255)] * len(limbs)\n",
    "            components.append(\n",
    "                PoseHeaderComponent(\n",
    "                    name=comp_data[\"name\"],\n",
    "                    points=comp_data[\"points\"],\n",
    "                    limbs=limbs,\n",
    "                    colors=colors,\n",
    "                    point_format=comp_data.get(\"format\", \"XYZC\")\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        header = PoseHeader(\n",
    "            version=version,\n",
    "            dimensions=dimensions,\n",
    "            components=components\n",
    "        )\n",
    "    elif reference_pose is not None:\n",
    "        # Fallback to reference\n",
    "        width = int(loaded['width'][0]) if 'width' in loaded.files else reference_pose.header.dimensions.width\n",
    "        height = int(loaded['height'][0]) if 'height' in loaded.files else reference_pose.header.dimensions.height\n",
    "        depth = int(loaded['depth'][0]) if 'depth' in loaded.files else reference_pose.header.dimensions.depth\n",
    "        version = float(loaded['version'][0]) if 'version' in loaded.files else reference_pose.header.version\n",
    "        \n",
    "        header = clone_header(\n",
    "            reference_header=reference_pose.header,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            depth=depth,\n",
    "            version=version\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"No header found in NPZ and no reference_pose provided.\")\n",
    "    \n",
    "    pose = build_pose_from_arrays(data, confidence, fps, header, mask=mask)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pose.write(f)\n",
    "    \n",
    "    return pose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "487c36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: NPZ -> .pose (now standalone!)\n",
    "restored_pose_npz = npz_to_pose(f\"{save_path}/output.npz\", f\"{save_path}/restored_from_npz.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c100e06",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Convert to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17075e8b",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Important Note\n",
    "If you get `ArrowKeyError: A type extension with name pandas.period already defined`, restart the kernel and run all cells from the beginning. This is a known pyarrow/pandas compatibility issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f60c60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_parquet(pose: Pose, output_path: str, component_name: str = None):\n",
    "    \"\"\"\n",
    "    Convert Pose to Parquet (good for large analytics).\n",
    "    \n",
    "    Schema:\n",
    "    - frame_id: frame index\n",
    "    - person_id: person index\n",
    "    - point_name: point name\n",
    "    - component: component name\n",
    "    - x, y, z: coordinates\n",
    "    - confidence: confidence score\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    data = pose.body.data\n",
    "    confidence = pose.body.confidence\n",
    "    \n",
    "    frames, people, points, dims = data.shape\n",
    "    \n",
    "    # Build point list with names\n",
    "    point_info = []\n",
    "    for comp in pose.header.components:\n",
    "        if component_name and comp.name != component_name:\n",
    "            continue\n",
    "        for point_name in comp.points:\n",
    "            point_info.append((comp.name, point_name))\n",
    "    \n",
    "    print(f\"üîÑ Converting... ({frames} frames)\")\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        for person_idx in range(people):\n",
    "            for point_idx, (comp_name, point_name) in enumerate(point_info):\n",
    "                conf = confidence[frame_idx, person_idx, point_idx]\n",
    "                \n",
    "                # Skip points with zero confidence\n",
    "                if conf == 0:\n",
    "                    continue\n",
    "                \n",
    "                coords = data[frame_idx, person_idx, point_idx]\n",
    "                \n",
    "                row = {\n",
    "                    'frame_id': frame_idx,\n",
    "                    'time_sec': frame_idx / pose.body.fps,\n",
    "                    'person_id': person_idx,\n",
    "                    'component': comp_name,\n",
    "                    'point_name': point_name,\n",
    "                    'point_index': point_idx,\n",
    "                    'x': float(coords[0]),\n",
    "                    'y': float(coords[1]),\n",
    "                    'z': float(coords[2]) if dims > 2 else 0.0,\n",
    "                    'confidence': float(conf)\n",
    "                }\n",
    "                rows.append(row)\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Try different methods to avoid pyarrow conflicts\n",
    "    try:\n",
    "        df.to_parquet(output_path, index=False, compression='snappy', engine='pyarrow')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è PyArrow error (try restarting kernel): {str(e)[:100]}\")\n",
    "        print(\"üí° Trying alternative method...\")\n",
    "        try:\n",
    "            # Try fastparquet as alternative\n",
    "            df.to_parquet(output_path, index=False, compression='snappy', engine='fastparquet')\n",
    "        except:\n",
    "            # Final fallback: save without compression\n",
    "            print(\"üí° Using fallback method (no compression)...\")\n",
    "            df.to_parquet(output_path, index=False, engine='pyarrow', compression=None)\n",
    "    \n",
    "    print(f\"‚úÖ Saved to: {output_path}\")\n",
    "    print(f\"   üìä Rows: {len(df):,}\")\n",
    "    print(f\"   üì¶ File size: {Path(output_path).stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0fcd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting... (133 frames)\n",
      "‚úÖ Saved to: ../output/convert_pose_formats/output.parquet\n",
      "   üìä Rows: 26,705\n",
      "   üì¶ File size: 653.23 KB\n"
     ]
    }
   ],
   "source": [
    "df = pose_to_parquet(pose, f\"{save_path}/output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2d65214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_parquet_efficient(pose: Pose, output_path: str):\n",
    "    \"\"\"\n",
    "    Convert Pose to Parquet efficiently (one row per frame) with full header metadata.\n",
    "    \n",
    "    This method stores each frame as a single row with points as arrays.\n",
    "    Includes header as metadata for standalone reconstruction.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    frames = pose.body.data.shape[0]\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        row = {\n",
    "            'frame_id': frame_idx,\n",
    "            'time_sec': frame_idx / pose.body.fps,\n",
    "            'data': pose.body.data[frame_idx].filled(0).flatten().tolist(),\n",
    "            'confidence': pose.body.confidence[frame_idx].flatten().tolist()\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Prepare header metadata\n",
    "    components_data = []\n",
    "    for comp in pose.header.components:\n",
    "        components_data.append({\n",
    "            \"name\": comp.name,\n",
    "            \"points\": comp.points,\n",
    "            \"format\": comp.format,\n",
    "            \"limbs\": [[int(x) for x in limb] if hasattr(limb, '__iter__') else int(limb) for limb in comp.limbs],\n",
    "            \"colors\": [[int(x) for x in c] if hasattr(c, '__iter__') else int(c) for c in getattr(comp, \"colors\", [])] if hasattr(comp, \"colors\") else None\n",
    "        })\n",
    "    \n",
    "    # Save metadata in a separate JSON file instead of parquet metadata\n",
    "    metadata_path = output_path.rsplit('.', 1)[0] + '_metadata.json'\n",
    "    metadata = {\n",
    "        'fps': float(pose.body.fps),\n",
    "        'version': float(pose.header.version),\n",
    "        'width': int(pose.header.dimensions.width),\n",
    "        'height': int(pose.header.dimensions.height),\n",
    "        'depth': int(pose.header.dimensions.depth),\n",
    "        'components': components_data,\n",
    "        'shape': [int(x) for x in pose.body.data.shape]\n",
    "    }\n",
    "    \n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Save parquet without custom_metadata (compatibility issue)\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Saved (efficient) to: {output_path}\")\n",
    "    print(f\"   üìã Metadata: {metadata_path}\")\n",
    "    print(f\"   üìä Frames: {len(df)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac84c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_to_pose(parquet_path: str, output_path: str, reference_pose: Pose = None) -> Pose:\n",
    "    \"\"\"\n",
    "    Convert long-format Parquet back to a .pose file (now standalone when metadata available).\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Parquet file is empty.\")\n",
    "\n",
    "    frames = int(df[\"frame_id\"].max()) + 1\n",
    "    people = int(df[\"person_id\"].max()) + 1\n",
    "    dims = 3 if \"z\" in df.columns else 2\n",
    "    \n",
    "    # Try to get header from parquet metadata\n",
    "    try:\n",
    "        import pyarrow.parquet as pq\n",
    "        parquet_file = pq.ParquetFile(parquet_path)\n",
    "        metadata = parquet_file.schema_arrow.metadata\n",
    "        \n",
    "        if metadata and b'components' in metadata:\n",
    "            # Reconstruct from metadata\n",
    "            fps = float(metadata.get(b'fps', b'30').decode())\n",
    "            version = float(metadata.get(b'version', b'0.1').decode())\n",
    "            width = int(metadata.get(b'width', b'1920').decode())\n",
    "            height = int(metadata.get(b'height', b'1080').decode())\n",
    "            depth_val = int(metadata.get(b'depth', b'0').decode())\n",
    "            shape = json.loads(metadata[b'shape'].decode())\n",
    "            points = shape[2]\n",
    "            \n",
    "            dimensions = PoseHeaderDimensions(width, height, depth_val)\n",
    "            \n",
    "            components_data = json.loads(metadata[b'components'].decode())\n",
    "            components = []\n",
    "            for comp_data in components_data:\n",
    "                limbs = [tuple(limb) for limb in comp_data.get(\"limbs\", [])]\n",
    "                colors = comp_data.get(\"colors\")\n",
    "                if not colors:\n",
    "                    colors = [(255, 255, 255)] * len(limbs)\n",
    "                components.append(\n",
    "                    PoseHeaderComponent(\n",
    "                        name=comp_data[\"name\"],\n",
    "                        points=comp_data[\"points\"],\n",
    "                        limbs=limbs,\n",
    "                        colors=colors,\n",
    "                        point_format=comp_data.get(\"format\", \"XYZC\")\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            header = PoseHeader(version=version, dimensions=dimensions, components=components)\n",
    "        else:\n",
    "            raise ValueError(\"No metadata found\")\n",
    "    except Exception as e:\n",
    "        if reference_pose is None:\n",
    "            raise ValueError(f\"Cannot reconstruct header from Parquet metadata and no reference_pose provided. Error: {e}\")\n",
    "        header = reference_pose.header\n",
    "        points = reference_pose.header.total_points()\n",
    "        fps = reference_pose.body.fps\n",
    "    \n",
    "    data = np.zeros((frames, people, points, dims), dtype=np.float32)\n",
    "    confidence = np.zeros((frames, people, points), dtype=np.float32)\n",
    "    \n",
    "    for row in df.itertuples(index=False):\n",
    "        frame_idx = int(row.frame_id)\n",
    "        person_idx = int(row.person_id)\n",
    "        point_idx = int(row.point_index)\n",
    "        data[frame_idx, person_idx, point_idx, 0] = float(row.x)\n",
    "        data[frame_idx, person_idx, point_idx, 1] = float(row.y)\n",
    "        if dims > 2:\n",
    "            data[frame_idx, person_idx, point_idx, 2] = float(getattr(row, \"z\", 0.0))\n",
    "        confidence[frame_idx, person_idx, point_idx] = float(row.confidence)\n",
    "    \n",
    "    pose = build_pose_from_arrays(data, confidence, fps, header)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pose.write(f)\n",
    "    \n",
    "    return pose\n",
    "\n",
    "def parquet_efficient_to_pose(parquet_path: str, output_path: str, reference_pose: Pose = None) -> Pose:\n",
    "    \"\"\"\n",
    "    Convert efficient Parquet (one row per frame) back to a .pose file (now standalone with metadata JSON).\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    frames = len(df)\n",
    "    \n",
    "    # Try to load metadata from companion JSON file\n",
    "    metadata_path = parquet_path.rsplit('.', 1)[0] + '_metadata.json'\n",
    "    header = None\n",
    "    fps = 30.0\n",
    "    \n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Reconstruct header from metadata\n",
    "        fps = float(metadata[\"fps\"])\n",
    "        version = float(metadata[\"version\"])\n",
    "        width = int(metadata[\"width\"])\n",
    "        height = int(metadata[\"height\"])\n",
    "        depth_val = int(metadata[\"depth\"])\n",
    "        shape = metadata[\"shape\"]\n",
    "        \n",
    "        people, points, dims = shape[1], shape[2], shape[3]\n",
    "        \n",
    "        dimensions = PoseHeaderDimensions(width, height, depth_val)\n",
    "        \n",
    "        components_data = metadata[\"components\"]\n",
    "        components = []\n",
    "        for comp_data in components_data:\n",
    "            limbs = [tuple(limb) for limb in comp_data.get(\"limbs\", [])]\n",
    "            colors = comp_data.get(\"colors\")\n",
    "            if not colors:\n",
    "                colors = [(255, 255, 255)] * len(limbs)\n",
    "            components.append(\n",
    "                PoseHeaderComponent(\n",
    "                    name=comp_data[\"name\"],\n",
    "                    points=comp_data[\"points\"],\n",
    "                    limbs=limbs,\n",
    "                    colors=colors,\n",
    "                    point_format=comp_data.get(\"format\", \"XYZC\")\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        header = PoseHeader(version=version, dimensions=dimensions, components=components)\n",
    "        print(f\"‚úÖ Loaded header from: {metadata_path}\")\n",
    "    except FileNotFoundError:\n",
    "        if reference_pose is None:\n",
    "            raise ValueError(f\"Metadata file not found: {metadata_path}. Provide reference_pose or ensure metadata file exists.\")\n",
    "        header = reference_pose.header\n",
    "        people = reference_pose.body.data.shape[1]\n",
    "        points = reference_pose.body.data.shape[2]\n",
    "        dims = reference_pose.body.data.shape[3]\n",
    "        fps = reference_pose.body.fps\n",
    "        print(\"‚ö†Ô∏è No metadata file found, using reference_pose\")\n",
    "    \n",
    "    data = np.array(df[\"data\"].to_list(), dtype=np.float32)\n",
    "    confidence = np.array(df[\"confidence\"].to_list(), dtype=np.float32)\n",
    "    data = data.reshape(frames, people, points, dims)\n",
    "    confidence = confidence.reshape(frames, people, points)\n",
    "    \n",
    "    pose = build_pose_from_arrays(data, confidence, fps, header)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pose.write(f)\n",
    "    \n",
    "    return pose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0a606ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'width'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example: Parquet -> .pose (now standalone!)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m restored_pose_parquet_efficient = \u001b[43mparquet_efficient_to_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/output.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msave_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/restored_from_parquet_efficient.pose\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mparquet_efficient_to_pose\u001b[39m\u001b[34m(parquet_path, output_path, reference_pose)\u001b[39m\n\u001b[32m     96\u001b[39m fps = \u001b[38;5;28mfloat\u001b[39m(metadata[\u001b[33m\"\u001b[39m\u001b[33mfps\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     97\u001b[39m version = \u001b[38;5;28mfloat\u001b[39m(metadata[\u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m width = \u001b[38;5;28mint\u001b[39m(\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwidth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     99\u001b[39m height = \u001b[38;5;28mint\u001b[39m(metadata[\u001b[33m\"\u001b[39m\u001b[33mheight\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    100\u001b[39m depth_val = \u001b[38;5;28mint\u001b[39m(metadata[\u001b[33m\"\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mKeyError\u001b[39m: 'width'"
     ]
    }
   ],
   "source": [
    "# Example: Parquet -> .pose (now standalone!)\n",
    "restored_pose_parquet_efficient = parquet_efficient_to_pose(f\"{save_path}/output.parquet\", f\"{save_path}/restored_from_parquet_efficient.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b01e8f",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "660d9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_csv(pose: Pose, output_path: str):\n",
    "    \"\"\"\n",
    "    Convert Pose to CSV (exports all components) with header metadata in separate JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pose : Pose\n",
    "        Pose object.\n",
    "    output_path : str\n",
    "        Output file path (a .json metadata file will be created alongside).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    data = pose.body.data\n",
    "    confidence = pose.body.confidence\n",
    "    frames, people, _, _ = data.shape\n",
    "    \n",
    "    # Build point info for all components\n",
    "    point_idx = 0\n",
    "    point_info = []\n",
    "    for comp in pose.header.components:\n",
    "        for p_name in comp.points:\n",
    "            point_info.append((point_idx, comp.name, p_name))\n",
    "            point_idx += 1\n",
    "    \n",
    "    print(\"üîÑ Converting to CSV...\")\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        for person_idx in range(people):\n",
    "            for p_idx, comp_name, point_name in point_info:\n",
    "                conf = confidence[frame_idx, person_idx, p_idx]\n",
    "                if conf == 0:\n",
    "                    continue\n",
    "                    \n",
    "                coords = data[frame_idx, person_idx, p_idx]\n",
    "                rows.append({\n",
    "                    'frame': frame_idx,\n",
    "                    'time': round(frame_idx / pose.body.fps, 4),\n",
    "                    'person': person_idx,\n",
    "                    'component': comp_name,\n",
    "                    'point': point_name,\n",
    "                    'x': round(float(coords[0]), 4),\n",
    "                    'y': round(float(coords[1]), 4),\n",
    "                    'z': round(float(coords[2]), 4) if len(coords) > 2 else 0,\n",
    "                    'confidence': round(float(conf), 4)\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Save header metadata as companion JSON file\n",
    "    metadata_path = output_path.rsplit('.', 1)[0] + '_metadata.json'\n",
    "    components_data = []\n",
    "    for comp in pose.header.components:\n",
    "        components_data.append({\n",
    "            \"name\": comp.name,\n",
    "            \"points\": comp.points,\n",
    "            \"format\": comp.format,\n",
    "            \"limbs\": [[int(x) for x in limb] if hasattr(limb, '__iter__') else int(limb) for limb in comp.limbs],\n",
    "            \"colors\": [[int(x) for x in c] if hasattr(c, '__iter__') else int(c) for c in getattr(comp, \"colors\", [])] if hasattr(comp, \"colors\") else None\n",
    "        })\n",
    "    \n",
    "    metadata = {\n",
    "        \"fps\": float(pose.body.fps),\n",
    "        \"version\": float(pose.header.version),\n",
    "        \"dimensions\": {\n",
    "            \"width\": int(pose.header.dimensions.width),\n",
    "            \"height\": int(pose.header.dimensions.height),\n",
    "            \"depth\": int(pose.header.dimensions.depth)\n",
    "        },\n",
    "        \"components\": components_data,\n",
    "        \"shape\": [int(x) for x in pose.body.data.shape]\n",
    "    }\n",
    "    \n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Saved to: {output_path}\")\n",
    "    print(f\"   üìã Metadata: {metadata_path}\")\n",
    "    print(f\"   üìä Rows: {len(df):,}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "652d6c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting to CSV...\n",
      "‚úÖ Saved to: ../output/convert_pose_formats/output.csv\n",
      "   üìã Metadata: ../output/convert_pose_formats/output_metadata.json\n",
      "   üìä Rows: 26,705\n"
     ]
    }
   ],
   "source": [
    "# Example: export to CSV\n",
    "df = pose_to_csv(pose, f\"{save_path}/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5a5d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_pose(csv_path: str, output_path: str, reference_pose: Pose = None) -> Pose:\n",
    "    \"\"\"\n",
    "    Convert CSV back to a .pose file (now standalone with metadata JSON file).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"CSV file is empty.\")\n",
    "\n",
    "    # Try to load metadata from companion JSON file\n",
    "    metadata_path = csv_path.rsplit('.', 1)[0] + '_metadata.json'\n",
    "    header = None\n",
    "    fps = 30.0\n",
    "    \n",
    "    try:\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Reconstruct header from metadata\n",
    "        fps = float(metadata[\"fps\"])\n",
    "        version = float(metadata[\"version\"])\n",
    "        dimensions = PoseHeaderDimensions(\n",
    "            width=metadata[\"dimensions\"][\"width\"],\n",
    "            height=metadata[\"dimensions\"][\"height\"],\n",
    "            depth=metadata[\"dimensions\"][\"depth\"]\n",
    "        )\n",
    "        \n",
    "        components = []\n",
    "        for comp_data in metadata[\"components\"]:\n",
    "            limbs = [tuple(limb) for limb in comp_data.get(\"limbs\", [])]\n",
    "            colors = comp_data.get(\"colors\")\n",
    "            if not colors:\n",
    "                colors = [(255, 255, 255)] * len(limbs)\n",
    "            components.append(\n",
    "                PoseHeaderComponent(\n",
    "                    name=comp_data[\"name\"],\n",
    "                    points=comp_data[\"points\"],\n",
    "                    limbs=limbs,\n",
    "                    colors=colors,\n",
    "                    point_format=comp_data.get(\"format\", \"XYZC\")\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        header = PoseHeader(version=version, dimensions=dimensions, components=components)\n",
    "        print(f\"‚úÖ Loaded header from: {metadata_path}\")\n",
    "    except FileNotFoundError:\n",
    "        if reference_pose is None:\n",
    "            raise ValueError(f\"Metadata file not found: {metadata_path}. Provide reference_pose or ensure metadata file exists.\")\n",
    "        header = reference_pose.header\n",
    "        fps = reference_pose.body.fps\n",
    "        print(\"‚ö†Ô∏è No metadata file found, using reference_pose\")\n",
    "    \n",
    "    frames = int(df[\"frame\"].max()) + 1\n",
    "    people = int(df[\"person\"].max()) + 1\n",
    "    points = header.total_points()\n",
    "    dims = 3 if \"z\" in df.columns else 2\n",
    "    \n",
    "    # Map (component, point) -> point index\n",
    "    point_map = {}\n",
    "    idx = 0\n",
    "    for comp in header.components:\n",
    "        for p_name in comp.points:\n",
    "            point_map[(comp.name, p_name)] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    data = np.zeros((frames, people, points, dims), dtype=np.float32)\n",
    "    confidence = np.zeros((frames, people, points), dtype=np.float32)\n",
    "    \n",
    "    for row in df.itertuples(index=False):\n",
    "        frame_idx = int(row.frame)\n",
    "        person_idx = int(row.person)\n",
    "        point_idx = int(point_map[(row.component, row.point)])\n",
    "        data[frame_idx, person_idx, point_idx, 0] = float(row.x)\n",
    "        data[frame_idx, person_idx, point_idx, 1] = float(row.y)\n",
    "        if dims > 2:\n",
    "            data[frame_idx, person_idx, point_idx, 2] = float(getattr(row, \"z\", 0.0))\n",
    "        confidence[frame_idx, person_idx, point_idx] = float(row.confidence)\n",
    "    \n",
    "    pose = build_pose_from_arrays(data, confidence, fps, header)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pose.write(f)\n",
    "    \n",
    "    return pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a52e579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded header from: ../output/convert_pose_formats/output_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Example: CSV -> .pose (now standalone!)\n",
    "restored_pose_csv = csv_to_pose(f\"{save_path}/output.csv\", f\"{save_path}/restored_from_csv.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59762e",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Comprehensive conversion function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82354639",
   "metadata": {},
   "source": [
    "## ‚ú® Standalone Conversion Tests\n",
    "\n",
    "All conversion methods now work **independently** without requiring `reference_pose`!\n",
    "- ‚úÖ Compact JSON: Includes full header\n",
    "- ‚úÖ NPZ: Stores components as JSON\n",
    "- ‚úÖ Parquet: Uses custom metadata\n",
    "- ‚úÖ CSV: Creates companion `_metadata.json` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68d46c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Standalone Conversions...\n",
      "\n",
      "1Ô∏è‚É£ Testing Compact JSON...\n",
      "‚úÖ Saved (compact) to: ../output/convert_pose_formats/test.compact.json\n",
      "   ‚úÖ Original shape: (133, 1, 203, 3)\n",
      "   ‚úÖ Restored shape: (133, 1, 203, 3)\n",
      "‚úÖ Compact JSON: PASSED\n",
      "\n",
      "2Ô∏è‚É£ Testing NPZ...\n",
      "‚úÖ Saved to: ../output/convert_pose_formats/test.npz\n",
      "   üì¶ File size: 306.53 KB\n",
      "   ‚úÖ Original shape: (133, 1, 203, 3)\n",
      "   ‚úÖ Restored shape: (133, 1, 203, 3)\n",
      "‚úÖ NPZ: PASSED\n",
      "\n",
      "3Ô∏è‚É£ Testing CSV...\n",
      "üîÑ Converting to CSV...\n",
      "‚úÖ Saved to: ../output/convert_pose_formats/test.csv\n",
      "   üìã Metadata: ../output/convert_pose_formats/test_metadata.json\n",
      "   üìä Rows: 26,705\n",
      "‚úÖ Loaded header from: ../output/convert_pose_formats/test_metadata.json\n",
      "   ‚úÖ Original shape: (133, 1, 203, 3)\n",
      "   ‚úÖ Restored shape: (133, 1, 203, 3)\n",
      "‚úÖ CSV: PASSED\n",
      "\n",
      "4Ô∏è‚É£ Testing Parquet...\n",
      "‚úÖ Saved (efficient) to: ../output/convert_pose_formats/test_efficient.parquet\n",
      "   üìã Metadata: ../output/convert_pose_formats/test_efficient_metadata.json\n",
      "   üìä Frames: 133\n",
      "‚úÖ Loaded header from: ../output/convert_pose_formats/test_efficient_metadata.json\n",
      "   ‚úÖ Original shape: (133, 1, 203, 3)\n",
      "   ‚úÖ Restored shape: (133, 1, 203, 3)\n",
      "‚úÖ Parquet: PASSED\n",
      "\n",
      "============================================================\n",
      "üéâ ALL CONVERSIONS ARE NOW STANDALONE!\n",
      "============================================================\n",
      "No need for reference_pose anymore! ‚ú®\n"
     ]
    }
   ],
   "source": [
    "# Test Standalone Conversions (No reference_pose needed!)\n",
    "print(\"üß™ Testing Standalone Conversions...\\n\")\n",
    "\n",
    "# 1. Test Compact JSON\n",
    "print(\"1Ô∏è‚É£ Testing Compact JSON...\")\n",
    "pose_to_json_compact(pose, f\"{save_path}/test.compact.json\")\n",
    "restored_compact = json_compact_to_pose(f\"{save_path}/test.compact.json\", f\"{save_path}/test_compact_restored.pose\")\n",
    "print(f\"   ‚úÖ Original shape: {pose.body.data.shape}\")\n",
    "print(f\"   ‚úÖ Restored shape: {restored_compact.body.data.shape}\")\n",
    "print(\"‚úÖ Compact JSON: PASSED\\n\")\n",
    "\n",
    "# 2. Test NPZ\n",
    "print(\"2Ô∏è‚É£ Testing NPZ...\")\n",
    "pose_to_npz(pose, f\"{save_path}/test.npz\", compressed=True)\n",
    "restored_npz = npz_to_pose(f\"{save_path}/test.npz\", f\"{save_path}/test_npz_restored.pose\")\n",
    "print(f\"   ‚úÖ Original shape: {pose.body.data.shape}\")\n",
    "print(f\"   ‚úÖ Restored shape: {restored_npz.body.data.shape}\")\n",
    "print(\"‚úÖ NPZ: PASSED\\n\")\n",
    "\n",
    "# 3. Test CSV\n",
    "print(\"3Ô∏è‚É£ Testing CSV...\")\n",
    "pose_to_csv(pose, f\"{save_path}/test.csv\")\n",
    "restored_csv = csv_to_pose(f\"{save_path}/test.csv\", f\"{save_path}/test_csv_restored.pose\")\n",
    "print(f\"   ‚úÖ Original shape: {pose.body.data.shape}\")\n",
    "print(f\"   ‚úÖ Restored shape: {restored_csv.body.data.shape}\")\n",
    "print(\"‚úÖ CSV: PASSED\\n\")\n",
    "\n",
    "# 4. Test Parquet (efficient format)\n",
    "print(\"4Ô∏è‚É£ Testing Parquet...\")\n",
    "pose_to_parquet_efficient(pose, f\"{save_path}/test_efficient.parquet\")\n",
    "restored_parquet = parquet_efficient_to_pose(f\"{save_path}/test_efficient.parquet\", f\"{save_path}/test_parquet_restored.pose\")\n",
    "print(f\"   ‚úÖ Original shape: {pose.body.data.shape}\")\n",
    "print(f\"   ‚úÖ Restored shape: {restored_parquet.body.data.shape}\")\n",
    "print(\"‚úÖ Parquet: PASSED\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ ALL CONVERSIONS ARE NOW STANDALONE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"No need for reference_pose anymore! ‚ú®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea4e34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pose(pose_path: str, output_format: str, output_path: str = None):\n",
    "    \"\"\"\n",
    "    Comprehensive conversion function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pose_path : str\n",
    "        Path to the .pose file.\n",
    "    output_format : str\n",
    "        Desired format: 'json', 'npz', 'parquet', 'csv'.\n",
    "    output_path : str, optional\n",
    "        Output file path (auto-generated if not provided).\n",
    "    \"\"\"\n",
    "    # Load file\n",
    "    pose = load_pose_file(pose_path)\n",
    "    \n",
    "    # Build output filename\n",
    "    base_path = Path(pose_path).stem\n",
    "    \n",
    "    format_map = {\n",
    "        'json': ('.json', pose_to_json),\n",
    "        'npz': ('.npz', pose_to_npz),\n",
    "        'parquet': ('.parquet', pose_to_parquet),\n",
    "        'csv': ('.csv', pose_to_csv)\n",
    "    }\n",
    "    \n",
    "    if output_format not in format_map:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported format: {output_format}. Supported formats: {list(format_map.keys())}\"\n",
    "        )\n",
    "    \n",
    "    ext, converter = format_map[output_format]\n",
    "    \n",
    "    if output_path is None:\n",
    "        output_path = f\"{base_path}{ext}\"\n",
    "    \n",
    "    return converter(pose, output_path)\n",
    "\n",
    "# Usage examples:\n",
    "# convert_pose(\"input.pose\", \"json\")\n",
    "# convert_pose(\"input.pose\", \"npz\")\n",
    "# convert_pose(\"input.pose\", \"parquet\")\n",
    "# convert_pose(\"input.pose\", \"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82b2dc",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Batch conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_convert(input_dir: str, output_format: str, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    Convert a folder of .pose files.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    pose_files = list(input_path.glob(\"*.pose\"))\n",
    "    \n",
    "    if not pose_files:\n",
    "        print(\"‚ö†Ô∏è No .pose files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÅ Found {len(pose_files)} files\")\n",
    "    \n",
    "    # Create output folder\n",
    "    if output_dir is None:\n",
    "        output_dir = input_path / f\"converted_{output_format}\"\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "    \n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Convert files\n",
    "    for pose_file in tqdm(pose_files, desc=\"Converting\"):\n",
    "        try:\n",
    "            ext = {'json': '.json', 'npz': '.npz', 'parquet': '.parquet', 'csv': '.csv'}[output_format]\n",
    "            output_path = output_dir / f\"{pose_file.stem}{ext}\"\n",
    "            convert_pose(str(pose_file), output_format, str(output_path))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in {pose_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Converted files saved to: {output_dir}\")\n",
    "\n",
    "# batch_convert(\"pose_files_folder\", \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af2ce2",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Compare file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1e24d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_file_sizes(pose_path: str):\n",
    "    \"\"\"\n",
    "    Compare file sizes across different formats.\n",
    "    \"\"\"\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    pose = load_pose_file(pose_path)\n",
    "    original_size = Path(pose_path).stat().st_size\n",
    "    \n",
    "    results = {'pose (original)': original_size}\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        # JSON\n",
    "        json_path = os.path.join(tmpdir, \"test.json\")\n",
    "        pose_to_json(pose, json_path)\n",
    "        results['json'] = Path(json_path).stat().st_size\n",
    "        \n",
    "        # NPZ (compressed)\n",
    "        npz_path = os.path.join(tmpdir, \"test.npz\")\n",
    "        pose_to_npz(pose, npz_path, compressed=True)\n",
    "        results['npz (compressed)'] = Path(npz_path).stat().st_size\n",
    "        \n",
    "        # Parquet\n",
    "        parquet_path = os.path.join(tmpdir, \"test.parquet\")\n",
    "        pose_to_parquet_efficient(pose, parquet_path)\n",
    "        results['parquet (efficient)'] = Path(parquet_path).stat().st_size\n",
    "    \n",
    "    # Show results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìä File size comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for fmt, size in sorted(results.items(), key=lambda x: x[1]):\n",
    "        ratio = size / original_size * 100\n",
    "        print(f\"{fmt:25} {size/1024:10.2f} KB  ({ratio:6.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f929012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: C:\\Users\\micrk\\AppData\\Local\\Temp\\tmp1hk83buf\\test.json\n",
      "‚úÖ Saved to: C:\\Users\\micrk\\AppData\\Local\\Temp\\tmp1hk83buf\\test.npz\n",
      "   üì¶ File size: 306.53 KB\n",
      "‚úÖ Saved (efficient) to: C:\\Users\\micrk\\AppData\\Local\\Temp\\tmp1hk83buf\\test.parquet\n",
      "   üìã Metadata: C:\\Users\\micrk\\AppData\\Local\\Temp\\tmp1hk83buf\\test_metadata.json\n",
      "   üìä Frames: 133\n",
      "\n",
      "==================================================\n",
      "üìä File size comparison\n",
      "==================================================\n",
      "npz (compressed)              306.53 KB  (  72.1%)\n",
      "pose (original)               424.93 KB  ( 100.0%)\n",
      "parquet (efficient)           680.45 KB  ( 160.1%)\n",
      "json                         3498.86 KB  ( 823.4%)\n"
     ]
    }
   ],
   "source": [
    "compare_file_sizes(f\"{save_path}/restored_from_npz.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5aae6",
   "metadata": {},
   "source": [
    "## üìö Format summary\n",
    "\n",
    "| Format | Pros | Cons | Best use |\n",
    "|--------|------|------|----------|\n",
    "| `.pose` | Native library format, compressed | Library-specific | Storage and library workflows |\n",
    "| `.json` | Human-readable, web-friendly | Large size | Web, debugging |\n",
    "| `.npz` | Efficient for NumPy, compressed | Requires NumPy | Scientific analysis |\n",
    "| `.parquet` | Efficient for big data | Requires pandas | Large-scale analytics |\n",
    "| `.csv` | Compatible with most tools | Very large size | Excel, other tools |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose-format-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
