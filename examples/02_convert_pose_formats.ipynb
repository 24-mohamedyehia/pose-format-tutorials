{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
    "!pip install pose-format numpy pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5214778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pose_format import Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296cdc6",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù .pose Ø£ÙˆÙ„Ø§Ù‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pose_file(pose_path: str) -> Pose:\n",
    "    \"\"\"\n",
    "    ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù .pose\n",
    "    \"\"\"\n",
    "    with open(pose_path, 'rb') as f:\n",
    "        pose = Pose.read(f.read())\n",
    "    return pose\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù\n",
    "# pose = load_pose_file(\"path/to/your/file.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9d600",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Ø­ÙØ¸ ÙƒÙ…Ù„Ù .pose (Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ø£ØµÙ„ÙŠØ©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aac15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pose(pose: Pose, output_path: str):\n",
    "    \"\"\"\n",
    "    Ø­ÙØ¸ ÙƒÙ…Ù„Ù .pose\n",
    "    \"\"\"\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pose.write(f)\n",
    "    print(f\"âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ: {output_path}\")\n",
    "    \n",
    "# save_as_pose(pose, \"output.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b737d5",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_json(pose: Pose, output_path: str = None, include_header: bool = True):\n",
    "    \"\"\"\n",
    "    ØªØ­ÙˆÙŠÙ„ Pose Ø¥Ù„Ù‰ JSON\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pose : Pose\n",
    "        ÙƒØ§Ø¦Ù† Ø§Ù„Ù€ Pose\n",
    "    output_path : str, optional\n",
    "        Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ø§ØªØ¬. Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ­Ø¯Ø¯ØŒ ÙŠÙØ±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙÙ‚Ø·\n",
    "    include_header : bool\n",
    "        ØªØ¶Ù…ÙŠÙ† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù€ header\n",
    "    \"\"\"\n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù‚ÙˆØ§Ø¦Ù… Python\n",
    "    data = pose.body.data.filled(0).tolist()  # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ù€ 0\n",
    "    confidence = pose.body.confidence.tolist()\n",
    "    \n",
    "    result = {\n",
    "        \"data\": data,\n",
    "        \"confidence\": confidence,\n",
    "        \"fps\": pose.body.fps,\n",
    "        \"shape\": {\n",
    "            \"frames\": pose.body.data.shape[0],\n",
    "            \"people\": pose.body.data.shape[1],\n",
    "            \"points\": pose.body.data.shape[2],\n",
    "            \"dimensions\": pose.body.data.shape[3]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if include_header:\n",
    "        result[\"header\"] = {\n",
    "            \"version\": pose.header.version,\n",
    "            \"dimensions\": {\n",
    "                \"width\": pose.header.dimensions.width,\n",
    "                \"height\": pose.header.dimensions.height,\n",
    "                \"depth\": pose.header.dimensions.depth\n",
    "            },\n",
    "            \"components\": [\n",
    "                {\n",
    "                    \"name\": comp.name,\n",
    "                    \"points\": comp.points,\n",
    "                    \"format\": comp.format,\n",
    "                    \"limbs\": comp.limbs,\n",
    "                    \"num_points\": len(comp.points)\n",
    "                }\n",
    "                for comp in pose.header.components\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    if output_path:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        print(f\"âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ: {output_path}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# json_data = pose_to_json(pose, \"output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea87d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_json_compact(pose: Pose, output_path: str):\n",
    "    \"\"\"\n",
    "    ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ JSON Ù…Ø¶ØºÙˆØ· (Ø¨Ø¯ÙˆÙ† Ù…Ø³Ø§ÙØ§Øª)\n",
    "    \"\"\"\n",
    "    data = pose.body.data.filled(0).tolist()\n",
    "    confidence = pose.body.confidence.tolist()\n",
    "    \n",
    "    result = {\n",
    "        \"d\": data,\n",
    "        \"c\": confidence,\n",
    "        \"f\": pose.body.fps,\n",
    "        \"w\": pose.header.dimensions.width,\n",
    "        \"h\": pose.header.dimensions.height\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(result, f, separators=(',', ':'))\n",
    "    \n",
    "    print(f\"âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ (Ù…Ø¶ØºÙˆØ·) ÙÙŠ: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327d685",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ NumPy (.npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_npz(pose: Pose, output_path: str, compressed: bool = True):\n",
    "    \"\"\"\n",
    "    ØªØ­ÙˆÙŠÙ„ Pose Ø¥Ù„Ù‰ Ù…Ù„Ù NumPy (.npz)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pose : Pose\n",
    "        ÙƒØ§Ø¦Ù† Ø§Ù„Ù€ Pose\n",
    "    output_path : str\n",
    "        Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ø§ØªØ¬\n",
    "    compressed : bool\n",
    "        Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¶ØºØ· (ÙŠÙ‚Ù„Ù„ Ø§Ù„Ø­Ø¬Ù… Ù„ÙƒÙ† Ø£Ø¨Ø·Ø£ ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©/Ø§Ù„ÙƒØªØ§Ø¨Ø©)\n",
    "    \"\"\"\n",
    "    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    data = np.array(pose.body.data.filled(0), dtype=np.float32)\n",
    "    confidence = np.array(pose.body.confidence, dtype=np.float32)\n",
    "    mask = np.array(pose.body.data.mask, dtype=bool)\n",
    "    \n",
    "    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©\n",
    "    metadata = {\n",
    "        'fps': np.array([pose.body.fps]),\n",
    "        'width': np.array([pose.header.dimensions.width]),\n",
    "        'height': np.array([pose.header.dimensions.height]),\n",
    "        'depth': np.array([pose.header.dimensions.depth]),\n",
    "        'version': np.array([pose.header.version])\n",
    "    }\n",
    "    \n",
    "    # Ø­ÙØ¸\n",
    "    save_func = np.savez_compressed if compressed else np.savez\n",
    "    save_func(\n",
    "        output_path,\n",
    "        data=data,\n",
    "        confidence=confidence,\n",
    "        mask=mask,\n",
    "        **metadata\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ: {output_path}\")\n",
    "    print(f\"   ğŸ“¦ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {Path(output_path).stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# pose_to_npz(pose, \"output.npz\", compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npz_to_pose(npz_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù .npz ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
    "    \"\"\"\n",
    "    loaded = np.load(npz_path)\n",
    "    \n",
    "    result = {\n",
    "        'data': loaded['data'],\n",
    "        'confidence': loaded['confidence'],\n",
    "        'mask': loaded['mask'],\n",
    "        'fps': loaded['fps'][0],\n",
    "        'width': loaded['width'][0],\n",
    "        'height': loaded['height'][0],\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù†: {npz_path}\")\n",
    "    print(f\"   Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {result['data'].shape}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# data_dict = npz_to_pose(\"output.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c100e06",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_parquet(pose: Pose, output_path: str, component_name: str = None):\n",
    "    \"\"\"\n",
    "    ØªØ­ÙˆÙŠÙ„ Pose Ø¥Ù„Ù‰ Parquet (Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©)\n",
    "    \n",
    "    Ø§Ù„Ø¨Ù†ÙŠØ©:\n",
    "    - frame_id: Ø±Ù‚Ù… Ø§Ù„Ø¥Ø·Ø§Ø±\n",
    "    - person_id: Ø±Ù‚Ù… Ø§Ù„Ø´Ø®Øµ\n",
    "    - point_name: Ø§Ø³Ù… Ø§Ù„Ù†Ù‚Ø·Ø©\n",
    "    - component: Ø§Ø³Ù… Ø§Ù„Ù…ÙƒÙˆÙ†\n",
    "    - x, y, z: Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª\n",
    "    - confidence: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    data = pose.body.data\n",
    "    confidence = pose.body.confidence\n",
    "    \n",
    "    frames, people, points, dims = data.shape\n",
    "    \n",
    "    # Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù‚Ø§Ø· Ù…Ø¹ Ø£Ø³Ù…Ø§Ø¦Ù‡Ø§\n",
    "    point_info = []\n",
    "    for comp in pose.header.components:\n",
    "        if component_name and comp.name != component_name:\n",
    "            continue\n",
    "        for point_name in comp.points:\n",
    "            point_info.append((comp.name, point_name))\n",
    "    \n",
    "    print(f\"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„... ({frames} Ø¥Ø·Ø§Ø±)\")\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        for person_idx in range(people):\n",
    "            for point_idx, (comp_name, point_name) in enumerate(point_info):\n",
    "                conf = confidence[frame_idx, person_idx, point_idx]\n",
    "                \n",
    "                # ØªØ®Ø·ÙŠ Ø§Ù„Ù†Ù‚Ø§Ø· Ø°Ø§Øª Ø§Ù„Ø«Ù‚Ø© ØµÙØ±\n",
    "                if conf == 0:\n",
    "                    continue\n",
    "                \n",
    "                coords = data[frame_idx, person_idx, point_idx]\n",
    "                \n",
    "                row = {\n",
    "                    'frame_id': frame_idx,\n",
    "                    'time_sec': frame_idx / pose.body.fps,\n",
    "                    'person_id': person_idx,\n",
    "                    'component': comp_name,\n",
    "                    'point_name': point_name,\n",
    "                    'point_index': point_idx,\n",
    "                    'x': float(coords[0]),\n",
    "                    'y': float(coords[1]),\n",
    "                    'z': float(coords[2]) if dims > 2 else 0.0,\n",
    "                    'confidence': float(conf)\n",
    "                }\n",
    "                rows.append(row)\n",
    "    \n",
    "    # Ø¥Ù†Ø´Ø§Ø¡ DataFrame ÙˆØ­ÙØ¸Ù‡\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_parquet(output_path, index=False, compression='snappy')\n",
    "    \n",
    "    print(f\"âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ: {output_path}\")\n",
    "    print(f\"   ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {len(df):,}\")\n",
    "    print(f\"   ğŸ“¦ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {Path(output_path).stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# df = pose_to_parquet(pose, \"output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d65214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_parquet_efficient(pose: Pose, output_path: str):\n",
    "    \"\"\"\n",
    "    ØªØ­ÙˆÙŠÙ„ Pose Ø¥Ù„Ù‰ Parquet Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© (ØµÙ Ù„ÙƒÙ„ Ø¥Ø·Ø§Ø±)\n",
    "    \n",
    "    Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© ØªØ®Ø²Ù† ÙƒÙ„ Ø¥Ø·Ø§Ø± ÙƒØµÙ ÙˆØ§Ø­Ø¯ Ù…Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø· ÙƒÙ€ arrays\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    frames = pose.body.data.shape[0]\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        row = {\n",
    "            'frame_id': frame_idx,\n",
    "            'time_sec': frame_idx / pose.body.fps,\n",
    "            'data': pose.body.data[frame_idx].filled(0).flatten().tolist(),\n",
    "            'confidence': pose.body.confidence[frame_idx].flatten().tolist()\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ (Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø©) ÙÙŠ: {output_path}\")\n",
    "    print(f\"   ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª: {len(df)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b01e8f",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_to_csv(pose: Pose, output_path: str, component_filter: list = None):\n",
    "    \"\"\"\n",
    "    ØªØ­ÙˆÙŠÙ„ Pose Ø¥Ù„Ù‰ CSV\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pose : Pose\n",
    "        ÙƒØ§Ø¦Ù† Ø§Ù„Ù€ Pose\n",
    "    output_path : str\n",
    "        Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ø§ØªØ¬\n",
    "    component_filter : list, optional\n",
    "        Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙ‚Ø·\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    data = pose.body.data\n",
    "    confidence = pose.body.confidence\n",
    "    frames, people, _, _ = data.shape\n",
    "    \n",
    "    # Ø¨Ù†Ø§Ø¡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù‚Ø§Ø·\n",
    "    point_idx = 0\n",
    "    point_info = []\n",
    "    for comp in pose.header.components:\n",
    "        if component_filter and comp.name not in component_filter:\n",
    "            point_idx += len(comp.points)\n",
    "            continue\n",
    "        for p_name in comp.points:\n",
    "            point_info.append((point_idx, comp.name, p_name))\n",
    "            point_idx += 1\n",
    "    \n",
    "    print(f\"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ CSV...\")\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        for person_idx in range(people):\n",
    "            for p_idx, comp_name, point_name in point_info:\n",
    "                conf = confidence[frame_idx, person_idx, p_idx]\n",
    "                if conf == 0:\n",
    "                    continue\n",
    "                    \n",
    "                coords = data[frame_idx, person_idx, p_idx]\n",
    "                rows.append({\n",
    "                    'frame': frame_idx,\n",
    "                    'time': round(frame_idx / pose.body.fps, 4),\n",
    "                    'person': person_idx,\n",
    "                    'component': comp_name,\n",
    "                    'point': point_name,\n",
    "                    'x': round(float(coords[0]), 4),\n",
    "                    'y': round(float(coords[1]), 4),\n",
    "                    'z': round(float(coords[2]), 4) if len(coords) > 2 else 0,\n",
    "                    'confidence': round(float(conf), 4)\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ ÙÙŠ: {output_path}\")\n",
    "    print(f\"   ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {len(df):,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ù…Ø«Ø§Ù„: ØªØµØ¯ÙŠØ± Ø§Ù„ÙŠØ¯ÙŠÙ† ÙÙ‚Ø·\n",
    "# df = pose_to_csv(pose, \"hands_only.csv\", \n",
    "#                  component_filter=[\"LEFT_HAND_LANDMARKS\", \"RIGHT_HAND_LANDMARKS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59762e",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ Ø´Ø§Ù…Ù„Ø©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pose(pose_path: str, output_format: str, output_path: str = None):\n",
    "    \"\"\"\n",
    "    Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ Ø´Ø§Ù…Ù„Ø©\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pose_path : str\n",
    "        Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù€ .pose\n",
    "    output_format : str\n",
    "        Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: 'json', 'npz', 'parquet', 'csv'\n",
    "    output_path : str, optional\n",
    "        Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ø§ØªØ¬ (ÙŠÙÙˆÙ„Ù‘Ø¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ­Ø¯Ø¯)\n",
    "    \"\"\"\n",
    "    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù\n",
    "    pose = load_pose_file(pose_path)\n",
    "    \n",
    "    # ØªØ­Ø¯ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ø§ØªØ¬\n",
    "    base_path = Path(pose_path).stem\n",
    "    \n",
    "    format_map = {\n",
    "        'json': ('.json', pose_to_json),\n",
    "        'npz': ('.npz', pose_to_npz),\n",
    "        'parquet': ('.parquet', pose_to_parquet),\n",
    "        'csv': ('.csv', pose_to_csv)\n",
    "    }\n",
    "    \n",
    "    if output_format not in format_map:\n",
    "        raise ValueError(f\"ØµÙŠØºØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©: {output_format}. Ø§Ù„ØµÙŠØº Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©: {list(format_map.keys())}\")\n",
    "    \n",
    "    ext, converter = format_map[output_format]\n",
    "    \n",
    "    if output_path is None:\n",
    "        output_path = f\"{base_path}{ext}\"\n",
    "    \n",
    "    return converter(pose, output_path)\n",
    "\n",
    "# Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:\n",
    "# convert_pose(\"input.pose\", \"json\")\n",
    "# convert_pose(\"input.pose\", \"npz\")\n",
    "# convert_pose(\"input.pose\", \"parquet\")\n",
    "# convert_pose(\"input.pose\", \"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82b2dc",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø¯ÙØ¹ÙŠ (Batch Conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_convert(input_dir: str, output_format: str, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„ Ù…Ù† Ù…Ù„ÙØ§Øª .pose\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    pose_files = list(input_path.glob(\"*.pose\"))\n",
    "    \n",
    "    if not pose_files:\n",
    "        print(\"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª .pose\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ“ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(pose_files)} Ù…Ù„Ù\")\n",
    "    \n",
    "    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬\n",
    "    if output_dir is None:\n",
    "        output_dir = input_path / f\"converted_{output_format}\"\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "    \n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª\n",
    "    for pose_file in tqdm(pose_files, desc=\"Ø§Ù„ØªØ­ÙˆÙŠÙ„\"):\n",
    "        try:\n",
    "            ext = {'json': '.json', 'npz': '.npz', 'parquet': '.parquet', 'csv': '.csv'}[output_format]\n",
    "            output_path = output_dir / f\"{pose_file.stem}{ext}\"\n",
    "            convert_pose(str(pose_file), output_format, str(output_path))\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ {pose_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ ÙÙŠ: {output_dir}\")\n",
    "\n",
    "# batch_convert(\"pose_files_folder\", \"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af2ce2",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e24d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_file_sizes(pose_path: str):\n",
    "    \"\"\"\n",
    "    Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ù„ØµÙŠØº Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n",
    "    \"\"\"\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    pose = load_pose_file(pose_path)\n",
    "    original_size = Path(pose_path).stat().st_size\n",
    "    \n",
    "    results = {'pose (original)': original_size}\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        # JSON\n",
    "        json_path = os.path.join(tmpdir, \"test.json\")\n",
    "        pose_to_json(pose, json_path)\n",
    "        results['json'] = Path(json_path).stat().st_size\n",
    "        \n",
    "        # NPZ (Ù…Ø¶ØºÙˆØ·)\n",
    "        npz_path = os.path.join(tmpdir, \"test.npz\")\n",
    "        pose_to_npz(pose, npz_path, compressed=True)\n",
    "        results['npz (compressed)'] = Path(npz_path).stat().st_size\n",
    "        \n",
    "        # Parquet\n",
    "        parquet_path = os.path.join(tmpdir, \"test.parquet\")\n",
    "        pose_to_parquet_efficient(pose, parquet_path)\n",
    "        results['parquet (efficient)'] = Path(parquet_path).stat().st_size\n",
    "    \n",
    "    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for fmt, size in sorted(results.items(), key=lambda x: x[1]):\n",
    "        ratio = size / original_size * 100\n",
    "        print(f\"{fmt:25} {size/1024:10.2f} KB  ({ratio:6.1f}%)\")\n",
    "\n",
    "# compare_file_sizes(\"your_file.pose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5aae6",
   "metadata": {},
   "source": [
    "## ğŸ“š Ù…Ù„Ø®Øµ Ø§Ù„ØµÙŠØº\n",
    "\n",
    "| Ø§Ù„ØµÙŠØºØ© | Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª | Ø§Ù„Ø¹ÙŠÙˆØ¨ | Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø«Ù„ |\n",
    "|--------|----------|--------|------------------|\n",
    "| `.pose` | ØµÙŠØºØ© Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©ØŒ Ù…Ø¶ØºÙˆØ·Ø© | Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…ÙƒØªØ¨Ø© | Ø§Ù„ØªØ®Ø²ÙŠÙ† ÙˆØ§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ÙƒØªØ¨Ø© |\n",
    "| `.json` | Ù‚Ø±Ø§Ø¡Ø© Ø³Ù‡Ù„Ø©ØŒ Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„ÙˆÙŠØ¨ | Ø­Ø¬Ù… ÙƒØ¨ÙŠØ± | Ø§Ù„ÙˆÙŠØ¨ØŒ Ø§Ù„ØªØµØ­ÙŠØ­ |\n",
    "| `.npz` | ÙØ¹Ø§Ù„Ø© Ù„Ù„Ù€ NumPyØŒ Ù…Ø¶ØºÙˆØ·Ø© | ØªØ­ØªØ§Ø¬ NumPy | Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ù…ÙŠ |\n",
    "| `.parquet` | ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© | ØªØ­ØªØ§Ø¬ pandas | Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© |\n",
    "| `.csv` | Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ | Ø­Ø¬Ù… ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹ | ExcelØŒ Ø¨Ø±Ø§Ù…Ø¬ Ø£Ø®Ø±Ù‰ |\n",
    "\n",
    "### Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©\n",
    "Ø§Ù†ØªÙ‚Ù„ Ù„Ù„Ù€ Notebook Ø§Ù„ØªØ§Ù„ÙŠ Ù„ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© **Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª** ÙÙŠ Python Ùˆ JavaScript!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
